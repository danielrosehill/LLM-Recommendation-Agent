# AI Agent for Evaluating LLMs for Specific Tasks

> The user discusses a potential AI agent designed to assist in evaluating large language models (LLMs) for specific tasks, like sysadmin work. The agent would focus on practical criteria such as tool usage, cost, context window, and community feedback, as well as assembling exclusion and inclusion criteria for the models.

*Transcribed: 31 Dec 2025 14:12*

---

Okay, the purpose of this voice note is to capture some context data for an AI agent that I think would be immensely helpful for me and maybe for other people. I'm one of the many utilities that I'm just kind of writing out, trying to get going or at least jotting down the idea and thoughts for just before the year is out.

So, I've been using agentic AI tools, CLIs like Qwen, Gemini, Claude for the past few months for a lot of different projects and they're extremely useful as commands for managing a desktop. Managing a desktop, managing a home environment, managing remote servers, all just by executing bash commands and translating natural language, like install this, move this, organize these files but going way beyond that actually.

I can use them for like I'm in a situation where I download raw videos from my camera and say, you can say just about anything. Create, you know, separate folders for photos and videos and move them accordingly. If you find any 4K videos, downsize to 1080p and it's an amazing use case in my opinion. Just like any of these, they will just go, okay, great, MKDIR, move ffmpeg, done.

And this use case is super but in my opinion, it's not received much attention. Everyone's really focused on code, these agentic tools for their ability to create code in a repository.

Anyway, what I was looking for yesterday, so I've been using Claude for this and I was kind of thinking, been thinking the last few months, it would be brilliant if there was a way to check out all the other models that might be good for this application because Claude Opus is an insanely good model, but it's also very expensive and a lot of the reasoning is wasted on this kind of stuff, I think. Sometimes there is genuinely like a really hard problem I'm trying to debug on Linux and that's another place it shines. But more commonly, like for create a folder, move files, all you need is an agent that knows basic Linux syntax and has tool calling.

So, when I was looking yesterday at SWE-Bench and I was thinking, okay, there's been a bunch of interesting new agents and new models lately coming out of China, Qwen, MiniMax, DeepSeek. I was going through that list and thinking, which of these could be suitable for this job? And it was fairly easy to draw up a pretty precise spec that I think could probably be translated into a programmatic evaluation criteria such that an agent could actually just give me the list of what would work.

In other words, my process was going through them one by one and saying, okay, this looks maybe this could work. Does it have vision? No. What's the parameter count? Could I run this locally? No. So that's another one, that might be kind of a, that's another strand of this might be doing that process of can it run? Which in my case and I think for most people, it's actually probably fairly relatively simple in the sense that I have 12 gigabytes VRAM, I have AMD GPU, okay. What's the parameter size of this model? It's 300B. No. Wait, are there quantizations? Yes. Are there any quantizations that can run on ROCm? No. Answer, is this locally viable? No.

So that's the kind of research and logic that I'm looking for. Those would be the steps that I'm going through. Going on to Hugging Face, checking is there a quantization? And some are much so like it's fairly easy usually to reach those findings pretty quickly. Generally, things are either, yeah, totally doable, I can run a quantized version of it or no, it's a non-runner.

But for the most part I'm not interested in local models. I'm mostly looking at cloud APIs. So to give the evaluation criteria that I was looking at yesterday as an example, what are the I can split my evaluation criteria between must haves and nice to haves. So the must have for my use case here is agentic tool calling. If it can't run, if it can't use MCPs, use tools, it's not going to work for my use case. So that's an exclusion criteria.

The price is actually kind of tricky because as a metric to source because models don't really have prices per se. I'm talking about API inference, they're, but there are averages based on the inference providers. So I'm not sure how that can be done. But the bedrock of information that would be really useful for this and I've seen a couple of projects, and I'm always just surprised that there isn't like one meta project that even goes beyond this, but this one is a very, very good start. A database of AI models. Let's see. Yeah, models.dev, that's the one that I came across. Oh, SurrealDB.com. There's another one. The multi-model database for AI agents. No, sorry, that's a different thing. A lot of look alikes. It's very confusing.

Models.dev, however, is exactly what I was looking for. A data source that lists the models and, you know, has an API such that it could be exposed as an MCP tool. And this one, I'm sure the key information to be gleaned from this would be let's take a typical AI flow where this agentic tool could be used. I might send in screenshots of the SWE-Bench leaderboard. And just like mark up annotations next to Qwen, MiniMax, DeepSeek, etcetera. I could describe much more briefly than I've done in this voice note what the intended use case was and my research task to the model recommendation agent would be figure out, give me some recommendations or what could I try over the API.

What I would like it to do from that starting point is firstly assemble the exclusion criteria and inclusion criteria. It would be really cool, this is kind of the extension of the idea to have something like an actual ranking logic by which I could say, okay, if we can see that Claude 4.5 Opus or whatever is going to replace it in a few months is the best, the biggest and the best, but it's also really expensive and that has to kind of rank as a factor in the decision-making logic. I'd love to just run Claude all day over the API and not worry about cost, but I also probably spend, you know, 1,000 bucks a month if I did that. So that's a big negative against it in the ranking.

And if I provided those parameters at the start, like really what I'm looking for is something maybe 80% as capable as the top state of the art, much more cost effective and that can support, you know, high volume daily use and I'll just keep Claude for when I'm really working on big projects.

The other metric that could go into the ranking from SWE-Bench is they actually have most, a lot of the data here figured is ready to go for this project. They have a percent resolve, that's the key benchmark. And they have an average cost. So the average cost doesn't provide you with the API cost in cost per million tokens in and out, but it's a very useful, it's actually a very, very helpful figure because it's a simpler yard stick, it's just how much did it cost to finish the task. It's the cost per completion. Cost to solve the project. Sorry, I'm thinking aloud a bit here, which makes so much more sense.

So, the idea for my, for this app, if I were to do the AI model recommendation agent and apply it again to my use case, if I'm looking for, give me some recommendations for my development, coding development thing. And the reason I'm again that I'm thinking of this is that because in three months, the SWE-Bench will look totally different, will have new models, different models. So it's a kind of thing that I'd like to use this model, you know, once or twice a year. Rerun it to quickly do the evaluation for me and take out all these kind of all the manual work.

So in summary, again looking at this, at this leaderboard, maybe the AI agent could help to define your criteria. I'm looking at the SWE-Benchmark now and let's say I say 60%, that's the cutoff. DeepSeek V3.2 reasoner sits exactly at the cutoff at 60% today. But that's where I am. And that gives us a pretty clean break in that in that data source.

Agentic tool calling we said as a must. Vision is a big. Vision is not a must, but it's a high priority. It's very very helpful to be able to drop screenshots into a model for code generation editing or system administration and a lot of models don't have it. I'd say the majority of models actually that are going to be on this benchmark don't have it. So that's a significant weight.

And what else is there? Oh the context window is very significant. And finally, the knowledge cutoff is also massively significant. And the older the cutoff, the more emphasis there has to be on MCP tooling, you need to bring in Codex.

And finally, there's one more I'm just thinking again, just as these come to mind, there is search capability. Some models like Claude, I'm not sure actually via the API. I might have to swallow my words there. Integrate search. So when I'm looking at what I could use for what model I could use for a project, I'm looking at the benchmark and I'm looking up the models. Oh and the parameter count as I mentioned. I'm kind of looking, I'm kind of piecing together a composite of information.

The resolution score on the benchmark, the cost, the, oh, I forgot one more, but I'll just finish it, finish my thought here first. The tool usage, search, context window, knowledge cutoff and finally reputation among the community. I pay a lot of precedence. Oh, and the release date, how new it is. And finally the community feedback. Every model manufacturer is going to say that it's great and it's wonderful and it's achieved such and such on benchmarks, but it's more useful to hear what people have said in the real world. Reddit is a good clearing house for that.

So it might be in this agent having a search step for everyone it goes through in the short list saying a search query like Qwen really small. What has been the overall sentiment among the developer community, user community for using this for that that could be question one. Question two would be what's the feedback and reputation being specifically for using it for tasks like Daniel is considering, agentic system administration.

So that's a lot of detail in this particular note and I'm going to distill it down to some context data and basically trying to come up with an agent that I think not only will do that research because that, well, there's two potential implementations here. Less and more ambitious.

Less ambitious, one that does the research. I give it a instruction just as I've done now, it goes to the benchmark, it crawls the benchmark site. It has an integration maybe, I think actually this would be should be there for every run, Hugging Face MCP so it can pull in the tech parameters, like the model cards programmatically. A search MCP, search API tool calling itself of course for this agent. Reasoning, report generation, generating the report and that's the basis. That would already be immensely helpful. Give me that list, give me provide me with the report.

And the second one that I think would be the kind of stretch implementation would more elegant would be take everything that I've said in my, let's say the three minute prompts, less wordy than this one. You know what Daniel's trying to do, the user, Daniel, whoever. You know what the things you want are, you know what the deal breakers are, come up with a scoring system. Apply the scoring system, provide the results and then at the end of the document, results and methodology, describe how you've converted Daniel or the user's criteria into a formal, objective ranking methodology and that would then provide like full lineage for, here are the results. Here's the date, always the date should be captured and the date should be injected into the prompt for this agent programmatically.

So that's the full idea really. And this prioritization for this idea, utility is actually pretty useful for my work. So I'd say it's kind of a, I would rank it as one of my long-term ideas, probably fairly low on the necessity scale, but higher on the utility scale. Priority low. This is one of my back burner project ideas, but I wanted to document it, so there it is.
